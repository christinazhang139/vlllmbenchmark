#!/usr/bin/env python3
"""
vLLMåŸºç¡€æ¨ç†æ€§èƒ½å®éªŒ
ç›®æ ‡ï¼šå…¨é¢æµ‹è¯•vLLMçš„åŸºæœ¬æ¨ç†æ€§èƒ½å’Œç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
"""

from vllm import LLM, SamplingParams
import time
import torch
import psutil
import json
from datetime import datetime

def get_system_status():
    """è·å–å½“å‰ç³»ç»ŸçŠ¶æ€"""
    status = {}
    
    # GPUå†…å­˜ä½¿ç”¨
    if torch.cuda.is_available():
        status['gpu_memory_allocated'] = torch.cuda.memory_allocated() / (1024**3)  # GB
        status['gpu_memory_total'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        status['gpu_name'] = torch.cuda.get_device_name(0)
    
    # ç³»ç»Ÿå†…å­˜ä½¿ç”¨
    memory = psutil.virtual_memory()
    status['system_memory_used'] = memory.used / (1024**3)  # GB
    status['system_memory_total'] = memory.total / (1024**3)  # GB
    status['system_memory_percent'] = memory.percent
    
    return status

def print_system_status(label, status):
    """æ‰“å°ç³»ç»ŸçŠ¶æ€ä¿¡æ¯"""
    print(f"\nğŸ“Š {label}:")
    if 'gpu_name' in status:
        print(f"   GPU: {status['gpu_name']}")
        print(f"   GPUå†…å­˜: {status['gpu_memory_allocated']:.1f}GB / {status['gpu_memory_total']:.1f}GB")
    print(f"   ç³»ç»Ÿå†…å­˜: {status['system_memory_used']:.1f}GB / {status['system_memory_total']:.1f}GB ({status['system_memory_percent']:.1f}%)")

def main():
    print("=" * 70)
    print("vLLMåŸºç¡€æ¨ç†æ€§èƒ½å®éªŒ")
    print("=" * 70)
    print("ğŸ¯ ç›®æ ‡ï¼šæµ‹è¯•vLLMçš„æ¨ç†æ€§èƒ½ã€å†…å­˜ä½¿ç”¨å’Œæ‰¹å¤„ç†æ•ˆç‡")
    print("ğŸ“Š è¾“å‡ºï¼šå®Œæ•´çš„æ€§èƒ½åˆ†ææŠ¥å‘Šå’ŒJSONæ•°æ®æ–‡ä»¶")
    print()
    
    # å®éªŒç»“æœè®°å½•
    experiment_results = {
        'experiment_info': {
            'timestamp': datetime.now().isoformat(),
            'experiment_name': 'vLLM Basic Inference Performance Test',
            'description': 'Comprehensive test of vLLM inference performance and resource usage'
        },
        'system_info': {},
        'performance_metrics': {},
        'test_results': []
    }
    
    # è®°å½•åˆå§‹ç³»ç»ŸçŠ¶æ€
    initial_status = get_system_status()
    experiment_results['system_info'] = initial_status
    print_system_status("å®éªŒå¼€å§‹å‰çš„ç³»ç»ŸçŠ¶æ€", initial_status)
    
    # =================================================================
    # ç¬¬1æ­¥ï¼šé…ç½®æ¨ç†å‚æ•°
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬1æ­¥ï¼šé…ç½®æ¨ç†å‚æ•°")
    print("=" * 70)
    
    sampling_params = SamplingParams(
        temperature=0.8,    # æ§åˆ¶è¾“å‡ºçš„éšæœºæ€§å’Œåˆ›é€ æ€§
        top_p=0.95,        # æ ¸é‡‡æ ·ï¼Œæ§åˆ¶è¯æ±‡é€‰æ‹©èŒƒå›´
        max_tokens=50,     # é™åˆ¶è¾“å‡ºé•¿åº¦ä¾¿äºåˆ†æ
        stop=None          # ä¸è®¾ç½®åœæ­¢æ¡ä»¶
    )
    
    print("ğŸ“‹ æ¨ç†å‚æ•°é…ç½®ï¼š")
    print(f"   â€¢ temperature = {sampling_params.temperature}")
    print(f"     ä½œç”¨ï¼šæ§åˆ¶è¾“å‡ºçš„éšæœºæ€§ï¼Œ0=ç¡®å®šæ€§ï¼Œ1=é«˜éšæœºæ€§")
    print(f"   â€¢ top_p = {sampling_params.top_p}")
    print(f"     ä½œç”¨ï¼šåªè€ƒè™‘æ¦‚ç‡ç´¯è®¡åˆ°95%çš„è¯æ±‡ï¼Œæé«˜è¾“å‡ºè´¨é‡")
    print(f"   â€¢ max_tokens = {sampling_params.max_tokens}")
    print(f"     ä½œç”¨ï¼šé™åˆ¶è¾“å‡ºé•¿åº¦ï¼Œä¾¿äºæ€§èƒ½æµ‹è¯•å’Œç»“æœåˆ†æ")
    print("âœ… å‚æ•°é…ç½®å®Œæˆ")
    
    # ä¿å­˜å‚æ•°é…ç½®
    experiment_results['parameters'] = {
        'temperature': sampling_params.temperature,
        'top_p': sampling_params.top_p,
        'max_tokens': sampling_params.max_tokens
    }
    
    # =================================================================
    # ç¬¬2æ­¥ï¼šåŠ è½½æ¨¡å‹
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬2æ­¥ï¼šåŠ è½½æ¨¡å‹")
    print("=" * 70)
    
    model_name = "facebook/opt-125m"
    print(f"ğŸ“¦ é€‰æ‹©æ¨¡å‹ï¼š{model_name}")
    print("ğŸ” æ¨¡å‹ç‰¹ç‚¹ï¼š")
    print("   â€¢ å‚æ•°é‡ï¼š125,000,000 (1.25äº¿)")
    print("   â€¢ æ¨¡å‹ç±»å‹ï¼šå› æœè¯­è¨€æ¨¡å‹")
    print("   â€¢ å¼€å‘è€…ï¼šMeta (Facebook)")
    print("   â€¢ ä¼˜åŠ¿ï¼šä½“ç§¯å°ï¼ŒåŠ è½½å¿«ï¼Œé€‚åˆæ€§èƒ½åŸºå‡†æµ‹è¯•")
    
    # è®°å½•åŠ è½½å‰çš„å†…å­˜çŠ¶æ€
    pre_load_status = get_system_status()
    
    print(f"\nâ±ï¸  å¼€å§‹åŠ è½½æ¨¡å‹...")
    load_start_time = time.time()
    
    # åŠ è½½æ¨¡å‹
    llm = LLM(model=model_name)
    
    load_end_time = time.time()
    load_time = load_end_time - load_start_time
    
    # è®°å½•åŠ è½½åçš„å†…å­˜çŠ¶æ€
    post_load_status = get_system_status()
    memory_used = post_load_status['gpu_memory_allocated'] - pre_load_status['gpu_memory_allocated']
    
    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"   â±ï¸  åŠ è½½æ—¶é—´ï¼š{load_time:.2f}ç§’")
    print(f"   ğŸ’¾ GPUå†…å­˜å ç”¨ï¼š{memory_used:.1f}GB")
    print(f"   ğŸ“Š GPUå†…å­˜åˆ©ç”¨ç‡ï¼š{(post_load_status['gpu_memory_allocated']/post_load_status['gpu_memory_total']*100):.1f}%")
    
    print_system_status("æ¨¡å‹åŠ è½½åçš„ç³»ç»ŸçŠ¶æ€", post_load_status)
    
    # ä¿å­˜åŠ è½½æ€§èƒ½æ•°æ®
    experiment_results['performance_metrics']['model_loading'] = {
        'model_name': model_name,
        'load_time_seconds': load_time,
        'memory_used_gb': memory_used,
        'pre_load_gpu_memory_gb': pre_load_status['gpu_memory_allocated'],
        'post_load_gpu_memory_gb': post_load_status['gpu_memory_allocated']
    }
    
    # =================================================================
    # ç¬¬3æ­¥ï¼šå‡†å¤‡æµ‹è¯•æ•°æ®
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬3æ­¥ï¼šå‡†å¤‡æµ‹è¯•æ•°æ®")
    print("=" * 70)
    
    # è®¾è®¡å¤šæ ·åŒ–çš„æµ‹è¯•ç”¨ä¾‹ï¼Œè¦†ç›–ä¸åŒåº”ç”¨åœºæ™¯
    test_cases = [
        {
            "prompt": "Hello, my name is",
            "type": "ç®€å•ç»­å†™",
            "category": "basic_completion",
            "purpose": "æµ‹è¯•åŸºæœ¬çš„æ–‡æœ¬è¡¥å…¨èƒ½åŠ›"
        },
        {
            "prompt": "The weather today is",
            "type": "æè¿°æ€§æ–‡æœ¬",
            "category": "descriptive", 
            "purpose": "æµ‹è¯•æè¿°æ€§æ–‡æœ¬ç”Ÿæˆèƒ½åŠ›"
        },
        {
            "prompt": "Artificial intelligence is",
            "type": "æ¦‚å¿µè§£é‡Š",
            "category": "conceptual",
            "purpose": "æµ‹è¯•æ¦‚å¿µæ€§çŸ¥è¯†è¡¨è¾¾èƒ½åŠ›"
        },
        {
            "prompt": "In the future, technology will",
            "type": "é¢„æµ‹æ¨ç†",
            "category": "predictive",
            "purpose": "æµ‹è¯•é€»è¾‘æ¨ç†å’Œé¢„æµ‹èƒ½åŠ›"
        },
        {
            "prompt": "The best way to learn programming is",
            "type": "å»ºè®®æŒ‡å¯¼",
            "category": "instructional",
            "purpose": "æµ‹è¯•å»ºè®®å’ŒæŒ‡å¯¼å†…å®¹ç”Ÿæˆèƒ½åŠ›"
        }
    ]
    
    print("ğŸ“ æµ‹è¯•ç”¨ä¾‹è®¾è®¡ï¼š")
    print("   è®¾è®¡åŸåˆ™ï¼šè¦†ç›–AIåº”ç”¨çš„ä¸»è¦ä½¿ç”¨åœºæ™¯")
    for i, case in enumerate(test_cases, 1):
        print(f"   {i}. {case['type']}ï¼š\"{case['prompt']}\"")
        print(f"      ç›®çš„ï¼š{case['purpose']}")
        print(f"      åˆ†ç±»ï¼š{case['category']}")
    
    prompts = [case["prompt"] for case in test_cases]
    print(f"\nâœ… æµ‹è¯•æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(prompts)} ä¸ªæµ‹è¯•ç”¨ä¾‹")
    
    # ä¿å­˜æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯
    experiment_results['test_cases'] = test_cases
    
    # =================================================================
    # ç¬¬4æ­¥ï¼šæ‰§è¡Œæ¨ç†æµ‹è¯•
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬4æ­¥ï¼šæ‰§è¡Œæ¨ç†æµ‹è¯•")
    print("=" * 70)
    
    print("ğŸš€ å¼€å§‹æ‰¹é‡æ¨ç†...")
    print("ğŸ’¡ ç‰¹è‰²ï¼švLLMä½¿ç”¨æ‰¹å¤„ç†æŠ€æœ¯åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚")
    print("ğŸ“Š æµ‹è¯•ï¼šå°†åŒæ—¶å¤„ç†æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼Œæµ‹é‡æ•´ä½“æ€§èƒ½")
    
    # æ‰§è¡Œæ¨ç†å¹¶è®¡æ—¶
    inference_start_time = time.time()
    outputs = llm.generate(prompts, sampling_params)
    inference_end_time = time.time()
    
    inference_time = inference_end_time - inference_start_time
    
    print(f"âœ… æ¨ç†å®Œæˆ")
    print(f"   â±ï¸  æ¨ç†æ€»æ—¶é—´ï¼š{inference_time:.2f}ç§’")
    print(f"   ğŸ“ˆ å¤„ç†é€Ÿåº¦ï¼š{len(prompts)/inference_time:.2f} è¯·æ±‚/ç§’")
    print(f"   âš¡ å¹³å‡å»¶è¿Ÿï¼š{inference_time/len(prompts):.2f}ç§’/è¯·æ±‚")
    
    # ä¿å­˜æ¨ç†æ€§èƒ½æ•°æ®
    experiment_results['performance_metrics']['inference'] = {
        'total_time_seconds': inference_time,
        'throughput_requests_per_second': len(prompts) / inference_time,
        'average_latency_seconds': inference_time / len(prompts),
        'total_requests': len(prompts),
        'batch_processing': True
    }
    
    # =================================================================
    # ç¬¬5æ­¥ï¼šè¯¦ç»†ç»“æœåˆ†æ
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬5æ­¥ï¼šè¯¦ç»†ç»“æœåˆ†æ")
    print("=" * 70)
    
    print("ğŸ“‹ é€ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯¦ç»†ç»“æœï¼š")
    print("=" * 80)
    
    total_tokens = 0
    token_counts = []
    
    for i, (output, test_case) in enumerate(zip(outputs, test_cases)):
        prompt = output.prompt
        generated_text = output.outputs[0].text.strip()
        token_count = len(output.outputs[0].token_ids)
        total_tokens += token_count
        token_counts.append(token_count)
        
        # ä¿å­˜æ¯ä¸ªæµ‹è¯•çš„è¯¦ç»†ç»“æœ
        result = {
            'test_id': i + 1,
            'category': test_case['category'],
            'type': test_case['type'],
            'prompt': prompt,
            'generated_text': generated_text,
            'token_count': token_count,
            'full_output': f"{prompt}{generated_text}",
            'purpose': test_case['purpose']
        }
        experiment_results['test_results'].append(result)
        
        print(f"\næµ‹è¯•ç”¨ä¾‹ {i+1}ï¼š{test_case['type']} ({test_case['category']})")
        print(f"è¾“å…¥æç¤ºï¼š\"{prompt}\"")
        print(f"ç”Ÿæˆæ–‡æœ¬ï¼š\"{generated_text}\"")
        print(f"Tokenæ•°é‡ï¼š{token_count}")
        print(f"å®Œæ•´è¾“å‡ºï¼š\"{prompt}{generated_text}\"")
        print("-" * 60)
    
    # =================================================================
    # ç¬¬6æ­¥ï¼šæ€§èƒ½ç»Ÿè®¡å’Œåˆ†æ
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬6æ­¥ï¼šæ€§èƒ½ç»Ÿè®¡å’Œåˆ†æ")
    print("=" * 70)
    
    # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    avg_tokens_per_output = total_tokens / len(outputs)
    tokens_per_second = total_tokens / inference_time
    min_tokens = min(token_counts)
    max_tokens = max(token_counts)
    
    print("ğŸ“Š å…³é”®æ€§èƒ½æŒ‡æ ‡ (KPI)ï¼š")
    print(f"   ğŸƒ æ¨¡å‹åŠ è½½æ—¶é—´ï¼š{load_time:.2f}ç§’")
    print(f"   âš¡ æ¨ç†æ€»æ—¶é—´ï¼š{inference_time:.2f}ç§’")
    print(f"   ğŸ“ˆ ååé‡ï¼š{len(prompts)/inference_time:.2f} è¯·æ±‚/ç§’")
    print(f"   ğŸ¯ å¹³å‡å»¶è¿Ÿï¼š{inference_time/len(prompts):.2f}ç§’/è¯·æ±‚")
    print(f"   ğŸ“ Tokenç»Ÿè®¡ï¼š")
    print(f"      â€¢ æ€»Tokenæ•°ï¼š{total_tokens}")
    print(f"      â€¢ å¹³å‡Tokenæ•°ï¼š{avg_tokens_per_output:.1f}")
    print(f"      â€¢ TokenèŒƒå›´ï¼š{min_tokens} ~ {max_tokens}")
    print(f"   ğŸš€ Tokenç”Ÿæˆé€Ÿåº¦ï¼š{tokens_per_second:.1f} tokens/ç§’")
    print(f"   ğŸ’¾ å†…å­˜ä½¿ç”¨ï¼š{memory_used:.1f}GB GPUå†…å­˜")
    
    # æ•ˆç‡åˆ†æ
    memory_efficiency = memory_used / 0.125  # GB per billion parameters
    gpu_utilization = (post_load_status['gpu_memory_allocated'] / post_load_status['gpu_memory_total']) * 100
    
    print(f"\nğŸ“ˆ æ•ˆç‡åˆ†æï¼š")
    print(f"   ğŸ’¡ å†…å­˜æ•ˆç‡ï¼š{memory_efficiency:.1f}GB/Bå‚æ•°")
    print(f"   ğŸ“Š GPUåˆ©ç”¨ç‡ï¼š{gpu_utilization:.1f}%")
    print(f"   âš–ï¸  æ‰¹å¤„ç†ä¼˜åŠ¿ï¼šåŒæ—¶å¤„ç†{len(prompts)}ä¸ªè¯·æ±‚")
    
    # ä¿å­˜ç»Ÿè®¡æ•°æ®
    experiment_results['performance_metrics']['statistics'] = {
        'total_tokens_generated': total_tokens,
        'average_tokens_per_output': avg_tokens_per_output,
        'tokens_per_second': tokens_per_second,
        'min_tokens_per_output': min_tokens,
        'max_tokens_per_output': max_tokens,
        'memory_efficiency_gb_per_billion_params': memory_efficiency,
        'gpu_utilization_percent': gpu_utilization
    }
    
    # =================================================================
    # ç¬¬7æ­¥ï¼šåŸºå‡†æ•°æ®æ€»ç»“
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬7æ­¥ï¼šåŸºå‡†æ•°æ®æ€»ç»“")
    print("=" * 70)
    
    print("ğŸ† vLLMæ€§èƒ½åŸºå‡†æ•°æ®ï¼š")
    print(f"   æ¡†æ¶ç‰ˆæœ¬ï¼švLLM (å½“å‰å®‰è£…ç‰ˆæœ¬)")
    print(f"   æµ‹è¯•æ¨¡å‹ï¼š{model_name} (125Må‚æ•°)")
    print(f"   ç¡¬ä»¶å¹³å°ï¼š{initial_status.get('gpu_name', 'Unknown GPU')}")
    print(f"   GPUå†…å­˜ï¼š{initial_status['gpu_memory_total']:.0f}GB")
    print(f"   ")
    print(f"   ğŸ“Š å…³é”®æŒ‡æ ‡ï¼š")
    print(f"      â€¢ æ¨¡å‹åŠ è½½ï¼š{load_time:.2f}ç§’")
    print(f"      â€¢ æ¨ç†ååï¼š{len(prompts)/inference_time:.2f} req/s")
    print(f"      â€¢ Tokené€Ÿç‡ï¼š{tokens_per_second:.1f} tok/s")
    print(f"      â€¢ å†…å­˜å ç”¨ï¼š{memory_used:.1f}GB")
    print(f"      â€¢ å†…å­˜æ•ˆç‡ï¼š{memory_efficiency:.1f}GB/Bå‚æ•°")
    
    # ä¿å­˜åŸºå‡†æ€»ç»“
    experiment_results['benchmark_summary'] = {
        'framework': 'vLLM',
        'model': model_name,
        'model_size': '125M parameters',
        'hardware': initial_status.get('gpu_name', 'Unknown GPU'),
        'gpu_memory_total': initial_status['gpu_memory_total'],
        'key_metrics': {
            'load_time_seconds': load_time,
            'throughput_req_per_sec': len(prompts) / inference_time,
            'token_generation_rate_per_sec': tokens_per_second,
            'memory_usage_gb': memory_used,
            'memory_efficiency_gb_per_billion_params': memory_efficiency
        }
    }
    
    # =================================================================
    # ç¬¬8æ­¥ï¼šä¿å­˜å®éªŒç»“æœ
    # =================================================================
    print("\n" + "=" * 70)
    print("ç¬¬8æ­¥ï¼šä¿å­˜å®éªŒç»“æœ")
    print("=" * 70)
    
    # ç”Ÿæˆç»“æœæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    result_filename = f"vllm_basic_experiment_{timestamp}.json"
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    with open(result_filename, 'w', encoding='utf-8') as f:
        json.dump(experiment_results, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜ï¼š{result_filename}")
    print(f"ğŸ“Š æ•°æ®åŒ…å«ï¼š")
    print(f"   â€¢ å®Œæ•´çš„ç³»ç»Ÿä¿¡æ¯")
    print(f"   â€¢ è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡")
    print(f"   â€¢ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹çš„ç»“æœ")
    print(f"   â€¢ åŸºå‡†æ•°æ®æ€»ç»“")
    
    # =================================================================
    # å®éªŒå®Œæˆæ€»ç»“
    # =================================================================
    print("\n" + "=" * 70)
    print("ğŸ¯ å®éªŒå®Œæˆæ€»ç»“")
    print("=" * 70)
    
    print("âœ… å®éªŒæˆåŠŸå®Œæˆï¼")
    print(f"ğŸ’¡ ä¸»è¦å‘ç°ï¼š")
    print(f"   1. vLLMåŠ è½½125Mæ¨¡å‹è€—æ—¶ {load_time:.1f}ç§’ï¼Œå†…å­˜å ç”¨ {memory_used:.1f}GB")
    print(f"   2. æ‰¹å¤„ç†æ¨ç†é€Ÿåº¦è¾¾åˆ° {len(prompts)/inference_time:.1f} è¯·æ±‚/ç§’")
    print(f"   3. Tokenç”Ÿæˆé€Ÿåº¦ä¸º {tokens_per_second:.0f} tokens/ç§’")
    print(f"   4. GPUå†…å­˜åˆ©ç”¨ç‡ä¸º {gpu_utilization:.1f}%")
    print(f"   5. æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹éƒ½æˆåŠŸç”Ÿæˆäº†åˆç†çš„æ–‡æœ¬è¾“å‡º")
    
    print(f"\nğŸ“‹ æ•°æ®ç”¨é€”ï¼š")
    print(f"   â€¢ å¯ä½œä¸ºvLLMæ€§èƒ½åŸºå‡†å‚è€ƒ")
    print(f"   â€¢ å¯ç”¨äºä¸åŒé…ç½®çš„æ€§èƒ½å¯¹æ¯”")
    print(f"   â€¢ å¯ç”¨äºç¡¬ä»¶å‡çº§æ•ˆæœè¯„ä¼°")
    print(f"   â€¢ å¯ç”¨äºä¸å…¶ä»–æ¨ç†æ¡†æ¶çš„å¯¹æ¯”åˆ†æ")
    
    print(f"\nğŸš€ åç»­å»ºè®®ï¼š")
    print(f"   â€¢ å°è¯•æ›´å¤§çš„æ¨¡å‹æµ‹è¯•æ€§èƒ½æ‰©å±•æ€§")
    print(f"   â€¢ æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°çš„æ€§èƒ½å·®å¼‚") 
    print(f"   â€¢ æ¯”è¾ƒä¸åŒæ¨ç†å‚æ•°çš„æ•ˆæœ")
    print(f"   â€¢ åœ¨ç›¸åŒæ¡ä»¶ä¸‹æµ‹è¯•å…¶ä»–æ¨ç†æ¡†æ¶")

if __name__ == "__main__":
    main()
